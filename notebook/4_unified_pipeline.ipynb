{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import date\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers.fix import OutputFixingParser\n",
    "from langchain_core.runnables import RunnableLambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output models using Pydantic v2\n",
    "class MICClassificationResult(BaseModel):\n",
    "    \"\"\"Schema for the Militarized Interstate Confrontation classification result.\"\"\"\n",
    "    is_mic: bool = Field(description=\"True if the article describes a Militarized Interstate Confrontation (MIC), False otherwise\")\n",
    "    explanation: str = Field(description=\"A brief explanation of why the article was classified as MIC or not\")\n",
    "\n",
    "class MICDetailedInfo(BaseModel):\n",
    "    \"\"\"Schema for detailed information about a Militarized Interstate Confrontation.\"\"\"\n",
    "    MICdate: date = Field(description=\"The date when the MIC occurred (YYYY-MM-DD format, or as specific as possible)\")\n",
    "    fatality_min: int = Field(description=\"The minimum number of fatalities (use same number as max if precise)\")\n",
    "    fatality_max: int = Field(description=\"The maximum number of fatalities (use same number as min if precise)\")\n",
    "    countries_involved: List[str] = Field(description=\"List of countries involved in the confrontation\")\n",
    "    initiator_country: Optional[str] = Field(description=\"The country that initiated the confrontation, if identifiable\")\n",
    "    target_country: Optional[str] = Field(description=\"The country that was targeted in the confrontation, if identifiable\")\n",
    "\n",
    "def init_llm() -> ChatOpenAI:\n",
    "    \"\"\"Initialize and return the LLM model using LangChain 0.3.\"\"\"\n",
    "    return ChatOpenAI(\n",
    "        base_url=\"http://localhost:1234/v1\",\n",
    "        api_key=\"LMStudio\",\n",
    "        model_name=\"qwen2.5-7b-instruct-1m\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "def create_classification_chain():\n",
    "    \"\"\"Create a LangChain classification chain with error fixing\"\"\"\n",
    "    llm = init_llm()\n",
    "    parser = PydanticOutputParser(pydantic_object=MICClassificationResult)\n",
    "    # Add output fixing parser to handle potential errors\n",
    "    fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)\n",
    "    \n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are an expert analyst of international relations and military conflicts.\n",
    "        Your task is to determine whether a news article describes a Militarized Interstate Confrontation (MIC).\n",
    "\n",
    "        A Militarized Interstate Confrontation (MIC) is defined as:\n",
    "        - A direct confrontation between two or more countries\n",
    "        - Involving military forces (army, navy, air force, etc.)\n",
    "        - Where there is a threat, display, or use of military force\n",
    "\n",
    "        The article must describe an actual military interaction, not just diplomatic tensions or discussions about potential conflicts.\n",
    "\n",
    "        Analyze the following article carefully and determine if it describes a MIC.\n",
    "        Output your answer in the specified JSON format with two fields:\n",
    "        1. is_mic: true if it's a MIC, false if it's not\n",
    "        2. explanation: A short explanation of your reasoning\n",
    "\n",
    "        Here are some examples to guide you:\n",
    "        Example 1:\n",
    "        Article: Russian troops opened fire on Ukrainian soldiers near the border, killing three and wounding seven others. The Ukrainian government condemned the attack as a violation of its sovereignty.\n",
    "        Output: {{\"is_mic\": true, \"explanation\": \"This article describes a direct military confrontation between Russian and Ukrainian forces with fatalities, which is a clear case of a Militarized Interstate Confrontation.\"}}\n",
    "\n",
    "        Example 2:\n",
    "        Article: China and Taiwan held diplomatic talks aimed at easing tensions in the region. Both sides agreed to maintain open lines of communication to prevent misunderstandings.\n",
    "        Output: {{\"is_mic\": false, \"explanation\": \"This article describes diplomatic talks rather than a military confrontation. No military forces were involved, and there was no threat or use of force.\"}}\n",
    "\n",
    "        Example 3:\n",
    "        Article: North Korean forces fired artillery shells into South Korean waters as a show of force during joint US-South Korean military exercises. No casualties were reported.\n",
    "        Output: {{\"is_mic\": true, \"explanation\": \"This article describes a militarized action (artillery fire) by North Korea directed at South Korea, which constitutes a Militarized Interstate Confrontation even without casualties.\"}}\n",
    "\n",
    "        Example 4:\n",
    "        Article: The United Nations Security Council met to discuss increasing tensions between India and Pakistan but no military actions were reported.\n",
    "        Output: {{\"is_mic\": false, \"explanation\": \"This article only mentions diplomatic discussions about tensions. It does not describe any actual military confrontation, threat, or use of force between countries.\"}}\n",
    "\n",
    "        Article:\n",
    "        {article}\n",
    "\n",
    "        {format_instructions}\n",
    "        \"\"\",\n",
    "        input_variables=['article'],\n",
    "        partial_variables={'format_instructions': format_instructions}\n",
    "    )\n",
    "    \n",
    "    # Build the LCEL chain with fixing parser\n",
    "    chain = prompt | llm | fixing_parser\n",
    "    \n",
    "    return chain\n",
    "\n",
    "def create_extraction_chain():\n",
    "    \"\"\"Create a LangChain chain for extracting detailed MIC information with error fixing.\"\"\"\n",
    "    llm = init_llm()\n",
    "    parser = PydanticOutputParser(pydantic_object=MICDetailedInfo)\n",
    "    # Add output fixing parser to handle potential errors\n",
    "    fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)\n",
    "    \n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are an expert analyst of international relations and military conflicts.\n",
    "        \n",
    "        This article has been identified as describing a Militarized Interstate Confrontation (MIC).\n",
    "        \n",
    "        Extract the following specific details about this confrontation:\n",
    "        1. The date when the confrontation occurred (be as precise as possible, use YYYY-MM-DD format if date is known else return 0000-00-00) \n",
    "        2. The number of fatalities (provide a range with minimum and maximum values; use the same number for both if precise)\n",
    "        3. All countries involved in the confrontation\n",
    "        4. If possible, identify which country initiated the confrontation and which was the target\n",
    "        \n",
    "        If any information is not explicitly stated in the article, make your best estimate based on context clues.\n",
    "        If you cannot determine a piece of information at all, use null for that field.\n",
    "\n",
    "        Example 1:\n",
    "        Article: \"On March 15, 2022, tensions escalated between Nation A and Nation B, leading to armed skirmishes. Reports confirm at least 50 casualties.\"\n",
    "        \n",
    "        Extracted Details:\n",
    "        ```\n",
    "        {{\n",
    "            \"MICdate\": \"2022-03-15\",\n",
    "            \"fatality_min\": 50,\n",
    "            \"fatality_max\": 50,\n",
    "            \"countries_involved\": [\"Nation A\", \"Nation B\"],\n",
    "            \"initiator_country\": \"Nation A\",\n",
    "            \"target_country\": \"Nation B\"\n",
    "        }}\n",
    "        ```\n",
    "        \n",
    "        Example 2:\n",
    "        Article: \"In early 1998, a naval conflict arose between Country X and Country Y. The exact number of casualties remains unknown.\"\n",
    "        \n",
    "        Extracted Details:\n",
    "        ```\n",
    "        {{\n",
    "            \"MICdate\": \"1998-01-01\",\n",
    "            \"fatality_min\": 0,\n",
    "            \"fatality_max\": 0,\n",
    "            \"countries_involved\": [\"Country X\", \"Country Y\"],\n",
    "            \"initiator_country\": null,\n",
    "            \"target_country\": null\n",
    "        }}\n",
    "        ```\n",
    "        \n",
    "        Article:\n",
    "        {article}\n",
    "        \n",
    "        {format_instructions}\n",
    "        \"\"\",\n",
    "        input_variables=['article'],\n",
    "        partial_variables={'format_instructions': format_instructions}\n",
    "    )\n",
    "    \n",
    "    # Build the LCEL chain with fixing parser\n",
    "    chain = prompt | llm | fixing_parser\n",
    "    \n",
    "    return chain\n",
    "\n",
    "def read_article_file(file_path: Path) -> Optional[str]:\n",
    "    \"\"\"Read an article file with proper error handling for encodings.\"\"\"\n",
    "    try:\n",
    "        # Try UTF-8 first\n",
    "        return file_path.read_text(encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # Fall back to Latin-1\n",
    "            return file_path.read_text(encoding=\"latin-1\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path.name}: {e}\")\n",
    "            return None\n",
    "\n",
    "def create_unified_chain():\n",
    "    \"\"\"Create a unified chain that handles both classification and extraction based on classification result.\"\"\"\n",
    "    classification_chain = create_classification_chain()\n",
    "    extraction_chain = create_extraction_chain()\n",
    "    \n",
    "    # Branch based on classification result\n",
    "    def process_classification_and_branch(inputs):\n",
    "        article_text = inputs[\"article_text\"]\n",
    "        file_index = inputs[\"file_index\"]\n",
    "        \n",
    "        # Classify the article\n",
    "        classification_result = classification_chain.invoke({\"article\": article_text})\n",
    "        \n",
    "        # If it's a MIC, extract details\n",
    "        if classification_result.is_mic:\n",
    "            extraction_result = extraction_chain.invoke({\"article\": article_text})\n",
    "            return {\n",
    "                \"file_index\": file_index,\n",
    "                \"classification_result\": classification_result,\n",
    "                \"extraction_result\": extraction_result,\n",
    "                \"is_mic\": True\n",
    "            }\n",
    "        \n",
    "        # If not a MIC, return only classification result\n",
    "        return {\n",
    "            \"file_index\": file_index,\n",
    "            \"classification_result\": classification_result,\n",
    "            \"extraction_result\": None,\n",
    "            \"is_mic\": False\n",
    "        }\n",
    "    \n",
    "    # Create the unified chain using a lambda function\n",
    "    chain = RunnableLambda(process_classification_and_branch)\n",
    "    \n",
    "    return chain\n",
    "\n",
    "def save_to_csv(file_index, result, csv_file, is_extraction=False):\n",
    "    \"\"\"Save results to CSV file, handling both classification and extraction results.\"\"\"\n",
    "    # Check if file exists to determine if headers are needed\n",
    "    file_exists = csv_file.exists()\n",
    "    \n",
    "    # Check if entry already exists in the file\n",
    "    existing_entries = set()\n",
    "    if file_exists:\n",
    "        with open(csv_file, mode=\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader, None)  # Skip header\n",
    "            for row in reader:\n",
    "                if row:  # Skip empty rows\n",
    "                    existing_entries.add(row[0])  # Add index to set\n",
    "    \n",
    "    # Skip if entry already exists\n",
    "    if file_index in existing_entries:\n",
    "        return False\n",
    "    \n",
    "    # Open file in appropriate mode\n",
    "    mode = \"a\" if file_exists else \"w\"\n",
    "    with open(csv_file, mode=mode, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # Write headers if it's a new file\n",
    "        if not file_exists:\n",
    "            if is_extraction:\n",
    "                writer.writerow([\n",
    "                    'Index', 'MICdate', 'Fatality_Min', 'Fatality_Max', \n",
    "                    'Countries_Involved', 'Initiator_Country', 'Target_Country'\n",
    "                ])\n",
    "            else:\n",
    "                writer.writerow(['Index', 'Label', 'Explanation'])\n",
    "        \n",
    "        # Write data row\n",
    "        if is_extraction:\n",
    "            writer.writerow([\n",
    "                file_index,\n",
    "                result.MICdate,\n",
    "                result.fatality_min,\n",
    "                result.fatality_max,\n",
    "                ', '.join(result.countries_involved) if result.countries_involved else '',\n",
    "                result.initiator_country if result.initiator_country else \"null\",\n",
    "                result.target_country if result.target_country else \"null\"\n",
    "            ])\n",
    "        else:\n",
    "            writer.writerow([\n",
    "                file_index,\n",
    "                int(result.is_mic),\n",
    "                result.explanation\n",
    "            ])\n",
    "    \n",
    "    return True\n",
    "\n",
    "def process_articles(base_dir: Path, output_dir: Path, years_to_process: list[str]) -> None:\n",
    "    \"\"\"Process articles using a unified approach that classifies and conditionally extracts details.\"\"\"\n",
    "    # Create output directories\n",
    "    classified_dir = output_dir / \"classified_files\"\n",
    "    detailed_dir = output_dir / \"detailed_files\"\n",
    "    classified_dir.mkdir(parents=True, exist_ok=True)\n",
    "    detailed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Initialize unified chain\n",
    "    unified_chain = create_unified_chain()\n",
    "    \n",
    "    # Process each year\n",
    "    for year in years_to_process:\n",
    "        year_dir = base_dir / year\n",
    "        \n",
    "        if not year_dir.is_dir():\n",
    "            print(f\"Directory for year {year} not found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Set up output files\n",
    "        classification_file = classified_dir / f\"{year}_classification.csv\"\n",
    "        extraction_file = detailed_dir / f\"{year}_mic_details.csv\"\n",
    "        \n",
    "        # Get all article files and sort them by name\n",
    "        article_files = sorted(list(year_dir.glob(\"**/*.txt\")), key=lambda x: x.name)\n",
    "        total_articles = len(article_files)\n",
    "        print(f\"Found {total_articles} articles in {year}\")\n",
    "        \n",
    "        # Check for existing processed entries\n",
    "        processed_entries = set()\n",
    "        if classification_file.exists():\n",
    "            with open(classification_file, 'r', newline='', encoding='utf-8') as f:\n",
    "                reader = csv.reader(f)\n",
    "                next(reader, None)  # Skip header\n",
    "                for row in reader:\n",
    "                    if row:\n",
    "                        processed_entries.add(row[0])\n",
    "            print(f\"Found {len(processed_entries)} already processed articles\")\n",
    "        \n",
    "        # Process articles with progress bar\n",
    "        with tqdm(total=total_articles, desc=f\"Processing {year}\", position=0) as pbar:\n",
    "            new_mic_count = 0\n",
    "            new_processed_count = 0\n",
    "            \n",
    "            for article_file in article_files:\n",
    "                # Create index for this file\n",
    "                file_index = f\"{year}_{article_file.name}\"\n",
    "                \n",
    "                # Skip if already processed\n",
    "                if file_index in processed_entries:\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Read the article\n",
    "                content = read_article_file(article_file)\n",
    "                if content is None:\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Process through unified chain\n",
    "                    result = unified_chain.invoke({\n",
    "                        \"article_text\": content,\n",
    "                        \"file_index\": file_index\n",
    "                    })\n",
    "                    \n",
    "                    # Save classification result\n",
    "                    classification_saved = save_to_csv(\n",
    "                        file_index, \n",
    "                        result[\"classification_result\"], \n",
    "                        classification_file\n",
    "                    )\n",
    "                    \n",
    "                    if classification_saved:\n",
    "                        new_processed_count += 1\n",
    "                    \n",
    "                    # If it's a MIC, save extraction result\n",
    "                    if result[\"is_mic\"] and result[\"extraction_result\"]:\n",
    "                        extraction_saved = save_to_csv(\n",
    "                            file_index,\n",
    "                            result[\"extraction_result\"],\n",
    "                            extraction_file,\n",
    "                            is_extraction=True\n",
    "                        )\n",
    "                        \n",
    "                        if extraction_saved:\n",
    "                            new_mic_count += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_index}: {str(e)[:100]}...\")\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Save progress every 10 articles\n",
    "                if new_processed_count % 10 == 0 and new_processed_count > 0:\n",
    "                    pbar.set_postfix({\"MICs\": new_mic_count, \"Processed\": new_processed_count})\n",
    "        \n",
    "        print(f\"Year {year} processing complete. New articles processed: {new_processed_count}, New MICs identified: {new_mic_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.cwd().parent / \"processed_files\"\n",
    "output_dir = Path.cwd().parent\n",
    "\n",
    "print(f\"Starting unified MIC classification and extraction pipeline\")\n",
    "process_articles(base_dir, output_dir, [\"2008\"])\n",
    "print(\"Processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
